{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 99546,
          "databundleVersionId": 11895149,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# üéØ Engage 2: Value from Clicks to Conversions\n\n## Problem Overview\nThis notebook analyzes user engagement data to predict purchase values from click-through data. The goal is to build a robust regression model that can accurately predict the purchase value based on various user interaction features.\n\n## Approach\n1. **Data Loading and Initial Exploration**\n2. **Comprehensive Exploratory Data Analysis (EDA)**\n3. **Data Preprocessing and Feature Engineering**\n4. **Model Development and Hyperparameter Tuning**\n5. **Model Comparison and Performance Analysis**\n6. **Final Submission**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# 1. üóÇÔ∏è Importing Required Libraries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Data manipulation and analysis\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning libraries\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport xgboost as xgb\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.2f}'.format)\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(\"All libraries imported successfully!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 2. üì• Data Loading and Initial Setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check available data files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load datasets\nfile_path_train = '/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv'\nfile_path_test = '/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv'\nfile_path_submission = '/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv'\n\n# Load data into variables\ntrain_original = pd.read_csv(file_path_train)\ntest_original = pd.read_csv(file_path_test)\nsubmission_format = pd.read_csv(file_path_submission)\n\n# Create working copies\ntrain = train_original.copy()\ntest = test_original.copy()\nsubmission = submission_format.copy()\n\nprint(f\"‚úÖ Data loaded successfully!\")\nprint(f\"Training data shape: {train.shape}\")\nprint(f\"Test data shape: {test.shape}\")\nprint(f\"Submission format shape: {submission.shape}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 3. üìä Comprehensive Exploratory Data Analysis (EDA)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 3.1 Dataset Overview and Basic Statistics",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Display basic information about the dataset\nprint(\"=\" * 50)\nprint(\"TRAINING DATA OVERVIEW\")\nprint(\"=\" * 50)\nprint(f\"Shape: {train.shape}\")\nprint(f\"Memory usage: {train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\nprint(\"\\nData types:\")\nprint(train.dtypes.value_counts())\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\" * 50)\ndisplay(train.head())\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"STATISTICAL SUMMARY\")\nprint(\"=\" * 50)\ndisplay(train.describe())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.2 Target Variable Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze the target variable 'purchaseValue'\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Distribution of purchase values\naxes[0, 0].hist(train['purchaseValue'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\naxes[0, 0].set_title('Distribution of Purchase Values')\naxes[0, 0].set_xlabel('Purchase Value')\naxes[0, 0].set_ylabel('Frequency')\n\n# Box plot for outliers\naxes[0, 1].boxplot(train['purchaseValue'])\naxes[0, 1].set_title('Box Plot: Purchase Values')\naxes[0, 1].set_ylabel('Purchase Value')\n\n# Log transformation (if needed)\nlog_purchase = np.log1p(train['purchaseValue'])\naxes[1, 0].hist(log_purchase, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\naxes[1, 0].set_title('Log-transformed Purchase Values')\naxes[1, 0].set_xlabel('Log(Purchase Value + 1)')\naxes[1, 0].set_ylabel('Frequency')\n\n# Purchase value statistics\nstats_text = f\"\"\"Purchase Value Statistics:\n‚Ä¢ Mean: ${train['purchaseValue'].mean():.2f}\n‚Ä¢ Median: ${train['purchaseValue'].median():.2f}\n‚Ä¢ Std Dev: ${train['purchaseValue'].std():.2f}\n‚Ä¢ Min: ${train['purchaseValue'].min():.2f}\n‚Ä¢ Max: ${train['purchaseValue'].max():.2f}\n‚Ä¢ Skewness: {train['purchaseValue'].skew():.2f}\n‚Ä¢ Zero Values: {(train['purchaseValue'] == 0).sum()}\"\"\"\n\naxes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, \n                fontsize=12, verticalalignment='center', \n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\naxes[1, 1].set_title('Purchase Value Statistics')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.3 Missing Data Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Comprehensive missing data analysis\nprint(\"=\" * 60)\nprint(\"MISSING DATA ANALYSIS\")\nprint(\"=\" * 60)\n\n# Check for 'not available in demo dataset' values\nzero_value_cols = []\npartial_missing_cols = []\n\nfor col in train.columns:\n    if train[col].dtype == 'object':  # Only check text columns\n        not_available = (train[col] == 'not available in demo dataset').sum()\n        if not_available == train.shape[0]:  # All values are 'not available'\n            zero_value_cols.append(col)\n        elif not_available > 0:  # Some values are 'not available'\n            partial_missing_cols.append((col, not_available))\n            print(f\"üìä {col}: {not_available} missing values ({not_available/train.shape[0]*100:.1f}%)\")\n\nprint(f\"\\nüóëÔ∏è Completely empty columns ({len(zero_value_cols)}): {zero_value_cols}\")\n\n# Regular missing values\nmissing_data = train.isnull().sum()\nmissing_percent = (missing_data / len(train)) * 100\nmissing_df = pd.DataFrame({\n    'Missing Count': missing_data,\n    'Missing Percentage': missing_percent\n}).sort_values('Missing Percentage', ascending=False)\n\nprint(\"\\nüìà Regular Missing Values:\")\nprint(missing_df[missing_df['Missing Count'] > 0])\n\n# Visualize missing data\nif missing_df['Missing Count'].sum() > 0:\n    plt.figure(figsize=(12, 6))\n    missing_cols = missing_df[missing_df['Missing Count'] > 0].head(10)\n    plt.bar(range(len(missing_cols)), missing_cols['Missing Percentage'])\n    plt.title('Missing Data Percentage by Column')\n    plt.xlabel('Columns')\n    plt.ylabel('Missing Percentage (%)')\n    plt.xticks(range(len(missing_cols)), missing_cols.index, rotation=45)\n    plt.tight_layout()\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.4 Data Cleaning",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Remove completely empty columns\nprint(f\"üßπ Removing {len(zero_value_cols)} completely empty columns...\")\nfor col in zero_value_cols:\n    train.drop(columns=[col], inplace=True)\n    test.drop(columns=[col], inplace=True)\n\n# Remove columns with very high missing percentages (>70%)\nhigh_missing_cols = ['trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot',\n                     'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.adNetworkType',\n                     'trafficSource.adwordsClickInfo.page']\n\nprint(f\"üßπ Removing {len(high_missing_cols)} high missing percentage columns...\")\nfor col in high_missing_cols:\n    if col in train.columns:\n        train.drop(columns=[col], inplace=True)\n        test.drop(columns=[col], inplace=True)\n\n# Remove unique identifier columns\nid_cols = ['sessionId', 'userId']\nprint(f\"üßπ Removing {len(id_cols)} identifier columns...\")\nfor col in id_cols:\n    if col in train.columns:\n        train.drop(columns=[col], inplace=True)\n        test.drop(columns=[col], inplace=True)\n\nprint(f\"\\n‚úÖ Data cleaning complete!\")\nprint(f\"New training data shape: {train.shape}\")\nprint(f\"New test data shape: {test.shape}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.5 Feature Type Identification",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Correctly identify feature types\nnumerical_features = train.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\n# Remove target variable from features\nif 'purchaseValue' in numerical_features:\n    numerical_features.remove('purchaseValue')\n\nprint(\"=\" * 60)\nprint(\"FEATURE TYPE IDENTIFICATION\")\nprint(\"=\" * 60)\n\nprint(f\"üìä Numerical Features ({len(numerical_features)}):\")\nfor i, feature in enumerate(numerical_features, 1):\n    print(f\"  {i:2d}. {feature}\")\n\nprint(f\"\\nüìù Categorical Features ({len(categorical_features)}):\")\nfor i, feature in enumerate(categorical_features, 1):\n    print(f\"  {i:2d}. {feature}\")\n\n# Check cardinality of categorical features\nprint(f\"\\nüìà Categorical Feature Cardinality:\")\ncardinality = train[categorical_features].nunique().sort_values(ascending=False)\nfor feature, count in cardinality.items():\n    print(f\"  ‚Ä¢ {feature}: {count} unique values\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.6 Correlation Analysis and Feature Relationships",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Correlation analysis for numerical features\nprint(\"=\" * 60)\nprint(\"CORRELATION ANALYSIS\")\nprint(\"=\" * 60)\n\n# Calculate correlation with target variable\nnumeric_cols = train.select_dtypes(include=[np.number]).columns\ncorr_with_target = train[numeric_cols].corr()['purchaseValue'].sort_values(ascending=False)\n\nprint(\"üìä Correlation with Purchase Value:\")\nfor feature, corr in corr_with_target.items():\n    if feature != 'purchaseValue':\n        print(f\"  ‚Ä¢ {feature}: {corr:.4f}\")\n\n# Visualize correlations\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# Correlation heatmap\ncorr_matrix = train[numeric_cols].corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, ax=axes[0], fmt='.2f')\naxes[0].set_title('Correlation Heatmap: Numerical Features')\n\n# Feature correlation with target\ntarget_corr = corr_with_target.drop('purchaseValue').sort_values()\naxes[1].barh(range(len(target_corr)), target_corr.values)\naxes[1].set_yticks(range(len(target_corr)))\naxes[1].set_yticklabels(target_corr.index, rotation=0)\naxes[1].set_xlabel('Correlation with Purchase Value')\naxes[1].set_title('Feature Correlation with Target Variable')\naxes[1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.7 Categorical Feature Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Analyze categorical features and their relationship with target\nprint(\"=\" * 60)\nprint(\"CATEGORICAL FEATURE ANALYSIS\")\nprint(\"=\" * 60)\n\n# Select key categorical features for analysis\nkey_categorical = ['deviceType', 'userChannel', 'screenSize']  # Add more as needed\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.flatten()\n\nfor i, feature in enumerate(key_categorical[:4]):\n    if feature in train.columns:\n        # Group by categorical feature and calculate mean purchase value\n        feature_analysis = train.groupby(feature)['purchaseValue'].agg(['count', 'mean', 'std']).reset_index()\n        feature_analysis = feature_analysis.sort_values('mean', ascending=False)\n        \n        print(f\"\\nüìä {feature} Analysis:\")\n        print(feature_analysis.head(10))\n        \n        # Plot\n        if i < 4:\n            top_categories = feature_analysis.head(10)\n            axes[i].bar(range(len(top_categories)), top_categories['mean'])\n            axes[i].set_title(f'Average Purchase Value by {feature}')\n            axes[i].set_xlabel(feature)\n            axes[i].set_ylabel('Average Purchase Value')\n            axes[i].set_xticks(range(len(top_categories)))\n            axes[i].set_xticklabels(top_categories[feature], rotation=45)\n\n# Hide unused subplots\nfor j in range(i+1, 4):\n    axes[j].axis('off')\n    \nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 3.8 Key Insights from EDA\n\n### üîç **Important Insights Learned:**\n\n1. **Target Variable Distribution**: \n   - Purchase values are highly skewed with many zero values\n   - May benefit from log transformation for some models\n   - Wide range of values suggests need for robust scaling\n\n2. **Missing Data Patterns**:\n   - Several columns completely empty (demo dataset limitation)\n   - Some features have systematic missing patterns\n   - Need careful imputation strategy\n\n3. **Feature Relationships**:\n   - [Add specific correlations found]\n   - Numerical features show [describe patterns]\n   - Categorical features reveal [describe insights]\n\n4. **Data Quality Issues**:\n   - High cardinality in some categorical features\n   - Need for feature engineering opportunities\n   - Potential for dimensionality reduction\n\n### üí° **Preprocessing Strategy**:\n- Handle missing values with appropriate imputation\n- Scale numerical features\n- Encode categorical features considering cardinality\n- Consider feature engineering for better performance",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# 4. üîß Data Preprocessing and Feature Engineering",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 4.1 Train-Test Split",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Split the data for model training and validation\nX = train.drop('purchaseValue', axis=1)\ny = train['purchaseValue']\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=None)\n\nprint(\"=\" * 50)\nprint(\"TRAIN-TEST SPLIT\")\nprint(\"=\" * 50)\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Validation set shape: {X_val.shape}\")\nprint(f\"Target training shape: {y_train.shape}\")\nprint(f\"Target validation shape: {y_val.shape}\")\nprint(f\"\\nTarget distribution in training set:\")\nprint(f\"Mean: {y_train.mean():.2f}\")\nprint(f\"Std: {y_train.std():.2f}\")\nprint(f\"Min: {y_train.min():.2f}\")\nprint(f\"Max: {y_train.max():.2f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4.2 Feature Engineering",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Feature Engineering Functions\ndef create_feature_interactions(df):\n    \"\"\"Create interaction features\"\"\"\n    df_new = df.copy()\n    \n    # Example interactions (adjust based on your features)\n    if 'pageViews' in df.columns and 'sessionNumber' in df.columns:\n        df_new['pageViews_per_session'] = df_new['pageViews'] / (df_new['sessionNumber'] + 1)\n    \n    if 'totalHits' in df.columns and 'pageViews' in df.columns:\n        df_new['hits_per_page'] = df_new['totalHits'] / (df_new['pageViews'] + 1)\n    \n    # Add time-based features if date column exists\n    if 'date' in df.columns:\n        df_new['date_squared'] = df_new['date'] ** 2\n        df_new['date_log'] = np.log1p(df_new['date'])\n    \n    return df_new\n\ndef create_aggregate_features(df):\n    \"\"\"Create aggregate features\"\"\"\n    df_new = df.copy()\n    \n    # Sum of related numerical features\n    numeric_cols = df_new.select_dtypes(include=[np.number]).columns\n    if len(numeric_cols) > 1:\n        df_new['numeric_sum'] = df_new[numeric_cols].sum(axis=1)\n        df_new['numeric_mean'] = df_new[numeric_cols].mean(axis=1)\n    \n    return df_new\n\nprint(\"üîß Feature Engineering Functions Created\")\nprint(\"Functions available:\")\nprint(\"  ‚Ä¢ create_feature_interactions()\")\nprint(\"  ‚Ä¢ create_aggregate_features()\")\nprint(\"\\nüí° These will be applied within the preprocessing pipeline\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 4.3 Preprocessing Pipeline Setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Enhanced preprocessing pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Custom transformer for feature engineering\"\"\"\n    \n    def __init__(self, create_interactions=True, create_aggregates=True):\n        self.create_interactions = create_interactions\n        self.create_aggregates = create_aggregates\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_new = X.copy()\n        \n        if self.create_interactions:\n            X_new = create_feature_interactions(X_new)\n        \n        if self.create_aggregates:\n            X_new = create_aggregate_features(X_new)\n            \n        return X_new\n\n# Define preprocessing for numerical features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Define preprocessing for categorical features\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\n# Create the complete preprocessing pipeline\ndef create_preprocessor(numerical_features, categorical_features):\n    \"\"\"Create a complete preprocessing pipeline\"\"\"\n    return Pipeline(steps=[\n        ('feature_engineering', FeatureEngineer()),\n        ('preprocessing', ColumnTransformer(\n            transformers=[\n                ('num', numerical_transformer, numerical_features),\n                ('cat', categorical_transformer, categorical_features)\n            ],\n            remainder='passthrough'\n        ))\n    ])\n\nprint(\"‚úÖ Preprocessing pipeline created successfully!\")\nprint(\"Pipeline includes:\")\nprint(\"  ‚Ä¢ Feature Engineering (interactions, aggregates)\")\nprint(\"  ‚Ä¢ Numerical feature preprocessing (imputation, scaling)\")\nprint(\"  ‚Ä¢ Categorical feature preprocessing (imputation, encoding)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 5. ü§ñ Model Development and Hyperparameter Tuning",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 5.1 Model Evaluation Framework",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Model evaluation framework\ndef evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n    \"\"\"Comprehensive model evaluation\"\"\"\n    \n    # Make predictions\n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    \n    # Calculate metrics\n    train_r2 = r2_score(y_train, y_train_pred)\n    val_r2 = r2_score(y_val, y_val_pred)\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    val_mae = mean_absolute_error(y_val, y_val_pred)\n    \n    # Create results dictionary\n    results = {\n        'Model': model_name,\n        'Train_R2': train_r2,\n        'Val_R2': val_r2,\n        'Train_RMSE': train_rmse,\n        'Val_RMSE': val_rmse,\n        'Train_MAE': train_mae,\n        'Val_MAE': val_mae,\n        'Overfitting': train_r2 - val_r2\n    }\n    \n    return results\n\nprint(\"üìä Model evaluation framework ready!\")\nprint(\"Metrics to be calculated:\")\nprint(\"  ‚Ä¢ R¬≤ Score (primary metric)\")\nprint(\"  ‚Ä¢ RMSE (Root Mean Square Error)\")\nprint(\"  ‚Ä¢ MAE (Mean Absolute Error)\")\nprint(\"  ‚Ä¢ Overfitting measure\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5.2 Model 1: XGBoost with Hyperparameter Tuning",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# XGBoost Model with Hyperparameter Tuning\nprint(\"=\" * 60)\nprint(\"üöÄ MODEL 1: XGBoost Regressor\")\nprint(\"=\" * 60)\n\n# Create preprocessing pipeline\npreprocessor_xgb = create_preprocessor(numerical_features, categorical_features)\n\n# Create XGBoost pipeline\nxgb_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor_xgb),\n    ('regressor', xgb.XGBRegressor(random_state=42))\n])\n\n# Hyperparameter grid for XGBoost\nxgb_param_grid = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__max_depth': [3, 5, 7],\n    'regressor__learning_rate': [0.01, 0.1, 0.2],\n    'regressor__subsample': [0.8, 0.9, 1.0],\n    'regressor__colsample_bytree': [0.8, 0.9, 1.0]\n}\n\n# Perform hyperparameter tuning\nprint(\"üîç Performing hyperparameter tuning...\")\nxgb_random_search = RandomizedSearchCV(\n    xgb_pipeline, \n    xgb_param_grid, \n    n_iter=20, \n    cv=3, \n    scoring='r2',\n    n_jobs=-1, \n    random_state=42,\n    verbose=1\n)\n\nxgb_random_search.fit(X_train, y_train)\n\n# Best XGBoost model\nbest_xgb = xgb_random_search.best_estimator_\n\nprint(f\"\\n‚úÖ Best XGBoost parameters:\")\nfor param, value in xgb_random_search.best_params_.items():\n    print(f\"  ‚Ä¢ {param}: {value}\")\n\n# Evaluate XGBoost model\nxgb_results = evaluate_model(best_xgb, X_train, X_val, y_train, y_val, \"XGBoost\")\n\nprint(f\"\\nüìä XGBoost Performance:\")\nfor metric, value in xgb_results.items():\n    if metric != 'Model':\n        print(f\"  ‚Ä¢ {metric}: {value:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5.3 Model 2: Extra Trees Regressor with Hyperparameter Tuning",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Extra Trees Regressor with Hyperparameter Tuning\nprint(\"=\" * 60)\nprint(\"üå≤ MODEL 2: Extra Trees Regressor\")\nprint(\"=\" * 60)\n\n# Create preprocessing pipeline for Extra Trees (using only numerical features)\npreprocessor_et = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features)\n    ],\n    remainder='drop'\n)\n\n# Create Extra Trees pipeline\net_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor_et),\n    ('regressor', ExtraTreesRegressor(random_state=42))\n])\n\n# Hyperparameter grid for Extra Trees\net_param_grid = {\n    'regressor__n_estimators': [100, 150, 200],\n    'regressor__max_depth': [None, 10, 20, 30],\n    'regressor__min_samples_split': [2, 5, 10],\n    'regressor__min_samples_leaf': [1, 2, 4],\n    'regressor__max_features': [0.2, 0.5, 0.8, 1.0]\n}\n\n# Perform hyperparameter tuning\nprint(\"üîç Performing hyperparameter tuning...\")\net_random_search = RandomizedSearchCV(\n    et_pipeline, \n    et_param_grid, \n    n_iter=20, \n    cv=3, \n    scoring='r2',\n    n_jobs=-1, \n    random_state=42,\n    verbose=1\n)\n\net_random_search.fit(X_train, y_train)\n\n# Best Extra Trees model\nbest_et = et_random_search.best_estimator_\n\nprint(f\"\\n‚úÖ Best Extra Trees parameters:\")\nfor param, value in et_random_search.best_params_.items():\n    print(f\"  ‚Ä¢ {param}: {value}\")\n\n# Evaluate Extra Trees model\net_results = evaluate_model(best_et, X_train, X_val, y_train, y_val, \"Extra Trees\")\n\nprint(f\"\\nüìä Extra Trees Performance:\")\nfor metric, value in et_results.items():\n    if metric != 'Model':\n        print(f\"  ‚Ä¢ {metric}: {value:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 5.4 Model 3: Random Forest Regressor with Hyperparameter Tuning",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Random Forest Regressor with Hyperparameter Tuning\nprint(\"=\" * 60)\nprint(\"üå≥ MODEL 3: Random Forest Regressor\")\nprint(\"=\" * 60)\n\n# Create preprocessing pipeline for Random Forest\npreprocessor_rf = create_preprocessor(numerical_features, categorical_features)\n\n# Create Random Forest pipeline\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor_rf),\n    ('regressor', RandomForestRegressor(random_state=42))\n])\n\n# Hyperparameter grid for Random Forest\nrf_param_grid = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__max_depth': [None, 10, 20, 30],\n    'regressor__min_samples_split': [2, 5, 10],\n    'regressor__min_samples_leaf': [1, 2, 4],\n    'regressor__max_features': ['sqrt', 'log2', None]\n}\n\n# Perform hyperparameter tuning\nprint(\"üîç Performing hyperparameter tuning...\")\nrf_random_search = RandomizedSearchCV(\n    rf_pipeline, \n    rf_param_grid, \n    n_iter=20, \n    cv=3, \n    scoring='r2',\n    n_jobs=-1, \n    random_state=42,\n    verbose=1\n)\n\nrf_random_search.fit(X_train, y_train)\n\n# Best Random Forest model\nbest_rf = rf_random_search.best_estimator_\n\nprint(f\"\\n‚úÖ Best Random Forest parameters:\")\nfor param, value in rf_random_search.best_params_.items():\n    print(f\"  ‚Ä¢ {param}: {value}\")\n\n# Evaluate Random Forest model\nrf_results = evaluate_model(best_rf, X_train, X_val, y_train, y_val, \"Random Forest\")\n\nprint(f\"\\nüìä Random Forest Performance:\")\nfor metric, value in rf_results.items():\n    if metric != 'Model':\n        print(f\"  ‚Ä¢ {metric}: {value:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# 6. üìä Model Comparison and Analysis",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 6.1 Comprehensive Model Comparison",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create comprehensive comparison\nprint(\"=\" * 80)\nprint(\"üìä COMPREHENSIVE MODEL COMPARISON\")\nprint(\"=\" * 80)\n\n# Combine all results\nall_results = [xgb_results, et_results, rf_results]\ncomparison_df = pd.DataFrame(all_results)\n\n# Display comparison table\nprint(\"\\nüìã Model Performance Comparison:\")\nprint(comparison_df.round(4))\n\n# Find best model based on validation R2\nbest_model_idx = comparison_df['Val_R2'].idxmax()\nbest_model_name = comparison_df.loc[best_model_idx, 'Model']\nbest_model_r2 = comparison_df.loc[best_model_idx, 'Val_R2']\n\nprint(f\"\\nüèÜ Best Model: {best_model_name} (Validation R¬≤: {best_model_r2:.4f})\")\n\n# Visualize model comparison\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# R¬≤ comparison\naxes[0, 0].bar(comparison_df['Model'], comparison_df['Val_R2'], color=['skyblue', 'lightcoral', 'lightgreen'])\naxes[0, 0].set_title('Model Comparison: Validation R¬≤ Score')\naxes[0, 0].set_ylabel('R¬≤ Score')\naxes[0, 0].set_ylim(0, 1)\n\n# RMSE comparison\naxes[0, 1].bar(comparison_df['Model'], comparison_df['Val_RMSE'], color=['skyblue', 'lightcoral', 'lightgreen'])\naxes[0, 1].set_title('Model Comparison: Validation RMSE')\naxes[0, 1].set_ylabel('RMSE')\n\n# Overfitting analysis\naxes[1, 0].bar(comparison_df['Model'], comparison_df['Overfitting'], color=['skyblue', 'lightcoral', 'lightgreen'])\naxes[1, 0].set_title('Model Comparison: Overfitting (Train R¬≤ - Val R¬≤)')\naxes[1, 0].set_ylabel('Overfitting Score')\naxes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n\n# Train vs Validation R¬≤ comparison\nmodels = comparison_df['Model']\nx_pos = np.arange(len(models))\nwidth = 0.35\n\naxes[1, 1].bar(x_pos - width/2, comparison_df['Train_R2'], width, label='Train R¬≤', color='lightblue')\naxes[1, 1].bar(x_pos + width/2, comparison_df['Val_R2'], width, label='Validation R¬≤', color='orange')\naxes[1, 1].set_title('Train vs Validation R¬≤ Score')\naxes[1, 1].set_ylabel('R¬≤ Score')\naxes[1, 1].set_xticks(x_pos)\naxes[1, 1].set_xticklabels(models)\naxes[1, 1].legend()\naxes[1, 1].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 6.2 Feature Importance Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Feature importance analysis for the best models\nprint(\"=\" * 80)\nprint(\"üîç FEATURE IMPORTANCE ANALYSIS\")\nprint(\"=\" * 80)\n\n# Function to plot feature importance\ndef plot_feature_importance(model, model_name, top_n=15):\n    \"\"\"Plot feature importance for a given model\"\"\"\n    try:\n        # Get the regressor from the pipeline\n        regressor = model.named_steps['regressor']\n        preprocessor = model.named_steps['preprocessor']\n        \n        # Get feature importances\n        importances = regressor.feature_importances_\n        \n        # Get feature names after preprocessing\n        feature_names = preprocessor.get_feature_names_out()\n        \n        # Create feature importance series\n        feature_importance = pd.Series(importances, index=feature_names)\n        \n        # Sort and get top features\n        top_features = feature_importance.sort_values(ascending=False).head(top_n)\n        \n        # Plot\n        plt.figure(figsize=(10, 8))\n        top_features.plot(kind='barh')\n        plt.title(f'Top {top_n} Feature Importances: {model_name}')\n        plt.xlabel('Importance Score')\n        plt.gca().invert_yaxis()\n        plt.tight_layout()\n        plt.show()\n        \n        # Print top features\n        print(f\"\\nüìä Top {top_n} Features for {model_name}:\")\n        for i, (feature, importance) in enumerate(top_features.items(), 1):\n            print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n            \n    except Exception as e:\n        print(f\"Could not plot feature importance for {model_name}: {e}\")\n\n# Plot feature importance for each model\nplot_feature_importance(best_xgb, \"XGBoost\")\nplot_feature_importance(best_et, \"Extra Trees\")\nplot_feature_importance(best_rf, \"Random Forest\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 6.3 Model Performance Insights",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### üéØ **Model Performance Analysis**\n\n#### **Key Findings:**\n\n1. **Best Performing Model**: \n   - [Model name] achieved the highest validation R¬≤ score of [value]\n   - Shows [low/moderate/high] overfitting with a difference of [value] between train and validation R¬≤\n\n2. **Model Comparison Insights**:\n   - **XGBoost**: [Add specific insights about XGBoost performance]\n   - **Extra Trees**: [Add specific insights about Extra Trees performance]\n   - **Random Forest**: [Add specific insights about Random Forest performance]\n\n3. **Feature Importance Insights**:\n   - Most important features consistently across models: [list top features]\n   - Model-specific important features: [describe differences]\n   - Feature engineering impact: [describe if engineered features are important]\n\n4. **Overfitting Analysis**:\n   - [Model name] shows the least overfitting\n   - [Model name] shows the most overfitting, suggesting need for regularization\n\n#### **Model Selection Rationale**:\n- Selected [model name] for final submission based on:\n  - Highest validation R¬≤ score\n  - Balanced performance (low overfitting)\n  - Robust to hyperparameter changes\n  - Good generalization capability\n\n#### **Potential Improvements**:\n- Further hyperparameter tuning with more iterations\n- Advanced feature engineering\n- Ensemble methods combining multiple models\n- Different preprocessing strategies for different models",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# 7. üöÄ Final Model Selection and Submission",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 7.1 Select Best Model and Retrain on Full Dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Select the best model based on validation R¬≤ score\nmodel_map = {\n    'XGBoost': best_xgb,\n    'Extra Trees': best_et,\n    'Random Forest': best_rf\n}\n\n# Get the best model\nbest_model = model_map[best_model_name]\n\nprint(\"=\" * 60)\nprint(\"üèÜ FINAL MODEL SELECTION\")\nprint(\"=\" * 60)\nprint(f\"Selected Model: {best_model_name}\")\nprint(f\"Validation R¬≤ Score: {best_model_r2:.4f}\")\n\n# Retrain the best model on the full dataset\nprint(f\"\\nüîÑ Retraining {best_model_name} on full dataset...\")\nbest_model.fit(X, y)\n\n# Make predictions on test set\nprint(\"üìã Making predictions on test set...\")\ntest_predictions = best_model.predict(test)\n\nprint(f\"\\n‚úÖ Predictions generated successfully!\")\nprint(f\"Test predictions shape: {test_predictions.shape}\")\nprint(f\"Test predictions range: {test_predictions.min():.2f} to {test_predictions.max():.2f}\")\nprint(f\"Test predictions mean: {test_predictions.mean():.2f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 7.2 Create Submission File",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create submission file\nsubmission_final = pd.DataFrame({\n    'id': np.arange(len(test_predictions)),\n    'purchaseValue': test_predictions\n})\n\n# Ensure no negative predictions (if needed)\nsubmission_final['purchaseValue'] = np.maximum(submission_final['purchaseValue'], 0)\n\n# Display submission statistics\nprint(\"=\" * 60)\nprint(\"üìä SUBMISSION STATISTICS\")\nprint(\"=\" * 60)\nprint(f\"Submission shape: {submission_final.shape}\")\nprint(f\"\\nPrediction Statistics:\")\nprint(f\"  ‚Ä¢ Mean: ${submission_final['purchaseValue'].mean():.2f}\")\nprint(f\"  ‚Ä¢ Median: ${submission_final['purchaseValue'].median():.2f}\")\nprint(f\"  ‚Ä¢ Min: ${submission_final['purchaseValue'].min():.2f}\")\nprint(f\"  ‚Ä¢ Max: ${submission_final['purchaseValue'].max():.2f}\")\nprint(f\"  ‚Ä¢ Std: ${submission_final['purchaseValue'].std():.2f}\")\nprint(f\"  ‚Ä¢ Zero predictions: {(submission_final['purchaseValue'] == 0).sum()}\")\n\n# Display first few rows\nprint(f\"\\nüìã First 10 rows of submission:\")\nprint(submission_final.head(10))\n\n# Save submission file\nsubmission_final.to_csv('/kaggle/working/submission.csv', index=False)\nprint(f\"\\n‚úÖ Submission file saved as 'submission.csv'\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## 7.3 Final Summary and Insights",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# üéØ **Project Summary and Key Insights**\n\n## **üìä Dataset Overview**\n- **Dataset Size**: [Training samples] training samples, [Test samples] test samples\n- **Features**: [Number] features after preprocessing\n- **Target**: Purchase value prediction (regression problem)\n- **Data Quality**: [Describe key data quality issues and how they were handled]\n\n## **üîç Key Insights from Analysis**\n\n### **1. Data Insights**\n- **Target Distribution**: [Describe distribution characteristics]\n- **Feature Relationships**: [Key correlations and patterns]\n- **Missing Data**: [Percentage and handling strategy]\n\n### **2. Model Performance**\n- **Best Model**: [Model name] with R¬≤ = [value]\n- **Model Ranking**: \n  1. [Model 1]: R¬≤ = [value]\n  2. [Model 2]: R¬≤ = [value] \n  3. [Model 3]: R¬≤ = [value]\n\n### **3. Feature Importance**\n- **Most Important Features**: [List top 5 features]\n- **Engineered Features**: [Impact of feature engineering]\n- **Preprocessing Impact**: [Effect of scaling and encoding]\n\n### **4. Model Insights**\n- **Overfitting**: [Which models overfit and why]\n- **Hyperparameter Impact**: [Most important hyperparameters]\n- **Generalization**: [Expected performance on unseen data]\n\n## **üöÄ Technical Approach**\n\n### **Preprocessing Pipeline**\n- ‚úÖ Missing value imputation\n- ‚úÖ Feature scaling for numerical features\n- ‚úÖ Categorical encoding\n- ‚úÖ Feature engineering\n- ‚úÖ Pipeline implementation\n\n### **Model Development**\n- ‚úÖ 3 different algorithms tested\n- ‚úÖ Hyperparameter tuning performed\n- ‚úÖ Cross-validation implemented\n- ‚úÖ Comprehensive evaluation metrics\n\n### **Best Practices Applied**\n- ‚úÖ Clean, well-commented code\n- ‚úÖ Proper train/validation split\n- ‚úÖ Pipeline usage for reproducibility\n- ‚úÖ Comprehensive model comparison\n- ‚úÖ Feature importance analysis\n\n## **üîÆ Future Improvements**\n1. **Advanced Feature Engineering**: Create more domain-specific features\n2. **Ensemble Methods**: Combine multiple models for better performance\n3. **Advanced Hyperparameter Tuning**: Use Bayesian optimization\n4. **Deep Learning**: Try neural networks for complex patterns\n5. **Cross-Validation**: Implement more sophisticated CV strategies\n\n## **üìà Expected Competition Performance**\nBased on validation R¬≤ of [value], we expect competitive performance in the leaderboard. The model shows good generalization with controlled overfitting."
    },
    {
      "cell_type": "code",
      "source": "# Final code cell for any additional analysis or verification\nprint(\"=\" * 80)\nprint(\"‚úÖ PROJECT COMPLETED SUCCESSFULLY!\")\nprint(\"=\" * 80)\nprint(f\"üìä Final Model: {best_model_name}\")\nprint(f\"üìà Validation R¬≤ Score: {best_model_r2:.4f}\")\nprint(f\"üìã Submission File: submission.csv\")\nprint(f\"üéØ Ready for Competition Submission!\")\nprint(\"=\" * 80)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
