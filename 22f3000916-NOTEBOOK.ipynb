{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99546,"databundleVersionId":11895149,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Engage 2: Value from Clicks to Conversions\n\n### Goal:\nThis notebook analyzes user engagement data to predict purchase values from click-through data. The goal is to build a robust regression model that can accurately predict the purchase value based on various user interaction features.\n\n### Content:\n1. *Data Loading and Setup*\n2. *Exploratory Data Analysis (EDA)*\n3. *Data Preprocessing and Feature Engineering*\n4. *Model Development and Hyperparameter Tuning*\n5. *Model Comparison and Performance Analysis*\n6. *Final Submission*","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Machine Learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import r2_score\nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Loading Data","metadata":{}},{"cell_type":"code","source":"# To check available data files\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading datasets\nfile_path_train = '/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv'\nfile_path_test = '/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv'\nfile_path_submission = '/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv'\n\n# Assigning Variables\ntrain_original = pd.read_csv(file_path_train)\ntest_original = pd.read_csv(file_path_test)\nsubmission_format = pd.read_csv(file_path_submission)\n\n# Working copies\ntrain = train_original.copy()\ntest = test_original.copy()\n\n\nprint(f\"Data loaded successfully!\\n\")\nprint(f\"Training data shape: {train.shape}\")\nprint(f\"Test data shape: {test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.float_format', '{:.2f}'.format) #for better readability ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1 Dataset Overview and Basic Statistics","metadata":{}},{"cell_type":"markdown","source":"### Feature Details\n\n**1. User Behavior & Session Metrics**\n  \n* *totalHits, pageViews, totals.bounces, new_visits, totals.visits:*\n\n  Indicators of user engagement and session activity.\n\n* *sessionNumber, sessionStart:*\n\n  Information related to session sequence and timing.\n\n**2. Device & Technical Attributes**\n* *deviceType, os, browser, screenSize, device.browserSize, device.language:*\n\n  Details about the user's device and browsing environment.\n\n* *browserMajor, device.:*\n\n    Encompasses a variety of device-level descriptors such as model, version, and screen specifications.\n\n* *gclIdPresent:*\n\n  Signals the presence of a Google Click ID used in ad tracking.\n\n**3. Traffic & Marketing Source**\n* *userChannel, trafficSource, trafficSource.medium, trafficSource.keyword, trafficSource.campaign:*\n\n  Insights into how users arrived at the platform.\n\n* *trafficSource.adwordsClickInfo.:*\n\n   Contains attributes from advertising sources, including ad network type and slot.\n\n* *trafficSource.adContent, trafficSource.referralPath, trafficSource.isTrueDirect:*\n\n  Provide further attribution details.\n\n**4. Geographical Context**\n* *geoNetwork.city, locationCountry, geoNetwork.continent, geoNetwork.subContinent, geoNetwork.metro, geoNetwork.region:*\n\n  Geographic identifiers to help understand regional behavior trends.\n  \n* *geoCluster, locationZone:*\n\n    Groupings based on geographic or behavioral patterns.\n\n**5. Identifiers**\n* *userId, sessionId:*\n\n  Unique identifiers for each user and session, allowing for multi-session analysis.\n**Target Variable**\n  \n* *purchaseValue:*\n\n  The amount (in currency units) spent by the customer during the session. This is the target variable to be predicted.","metadata":{}},{"cell_type":"code","source":"# Dataset shape and datatypes\nprint(\"=\" * 50)\nprint(\"-\"*12,\"Dataset Shape and Datatypes\",\"-\"*12)\nprint(\"=\" * 50)\nprint(f\"Shape: {train.shape}\")\nprint(\"\\nColumn Data types:\")\nprint(train.dtypes.value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First five rows\nprint(\"\\n\" + \"=\" * 50)\nprint(\"-\"*15,\"FIRST 5 ROWS\",\"-\"*15)\nprint(\"=\" * 50)\ndisplay(train.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset Descriptive stats\nprint(\"\\n\" + \"=\" * 50)\nprint(\"-\"*15,\"Descriptive Stats\",\"-\"*15)\nprint(\"=\" * 50)\ndisplay(train.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Target Variable Analysis","metadata":{}},{"cell_type":"code","source":"# Plot 1: Distribution of Purchase Values\nplt.figure(figsize=(10, 6))\nplt.hist(train['purchaseValue'], bins=50, color='skyblue', edgecolor='black')\nplt.title('Distribution of Purchase Values', fontsize=14, fontweight='bold')\nplt.xlabel('Purchase value (millions of unit)')\nplt.ylabel('Frequency (Number of Customers)')\nplt.grid(True, alpha=0.3)\nplt.yscale('log') #log scale used\nplt.show()\n\n# Print key statistics\nprint(\"Distribution Analysis:\\n\")\nprint(f\"Total customers: {len(train)}\")\nprint(f\"Zero purchases: {(train['purchaseValue'] == 0).sum()} ({(train['purchaseValue'] == 0).mean()*100:.1f}%)\")\nprint(f\"Non-zero purchases: {(train['purchaseValue'] > 0).sum()} ({(train['purchaseValue'] > 0).mean()*100:.1f}%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot 2: Box Plot for Outlier Detection\n\nnon_zero = train['purchaseValue'][train['purchaseValue'] > 0]\n\n# Box plot for full dataset\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.boxplot(train['purchaseValue'], vert=True)\nplt.ylabel('Purchase Value ($)')\nplt.title('All Customers (incl. zeros)')\n\n# Box plot for paying customers only, with log scale\nplt.subplot(1,2,2)\nplt.boxplot(non_zero, vert=True)\nplt.yscale('log')\nplt.ylabel('Purchase Value ($, log scale)')\nplt.title('Paying Customers (excl. zeros)')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"-\"*12,\"Box Plot Analysis:\",\"-\"*12)\nprint(\"=\" * 50)\n\n# Full dataset analysis (including zeros)\nq1_full = train['purchaseValue'].quantile(0.25)\nq3_full = train['purchaseValue'].quantile(0.75)\nmedian_full = train['purchaseValue'].quantile(0.50)\niqr_full = q3_full - q1_full\n\nprint(\"FULL DATASET (including zeros):\\n\")\nprint(f\" - Q1 (25th percentile): ${q1_full:.2f}\")\nprint(f\" - Median (50th percentile): ${median_full:.2f}\")\nprint(f\" - Q3 (75th percentile): ${q3_full:.2f}\")\nprint(f\" - IQR: ${iqr_full:.2f}\")\n\nprint(\" IQR = 0 because more than 75% of customers spent $0\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analysis for Paying Customers Only\nq1_buyers = non_zero.quantile(0.25)\nq3_buyers = non_zero.quantile(0.75)\nmedian_buyers = non_zero.quantile(0.50)\niqr_buyers = q3_buyers - q1_buyers\n\n# Calculating outlier bounds for paying customers\nlower_bound_buyers = q1_buyers - 1.5 * iqr_buyers\nupper_bound_buyers = q3_buyers + 1.5 * iqr_buyers\n\n# Find outliers among paying customers\nbuyer_outliers = non_zero[(non_zero < lower_bound_buyers) | (non_zero > upper_bound_buyers)]\nprint(\"PAYING CUSTOMERS ONLY (20.7% of customers):\\n\")\nprint(f\"  - Minimum paid purchase: ${non_zero.min()}\")\nprint(f\"  - Q1 (25th percentile): ${q1_buyers:,.2f}\")\nprint(f\"  - Median (50th percentile): ${median_buyers:,.2f}\")\nprint(f\"  - Q3 (75th percentile): ${q3_buyers:,.2f}\")\nprint(f\"  - IQR: ${iqr_buyers:,.2f}\")\nprint(f\"  - Lower outlier threshold: ${lower_bound_buyers:,.2f}\")\nprint(f\"  - Upper outlier threshold: ${upper_bound_buyers:,.2f}\")\n\n# Revenue analysis\ntotal_revenue = train['purchaseValue'].sum()\noutlier_revenue = buyer_outliers.sum()\n\nprint(f\"\\n > Revenue Impact:\")\nprint(f\"  - Total revenue: ${total_revenue:,.2f}\")\nprint(f\"  - Revenue from outliers: ${outlier_revenue:,.2f}\")\nprint(f\"  - Outliers contribute {outlier_revenue/total_revenue*100:.1f}% of total revenue\")\n\n# spending categories\nprint(f\"\\n > Spending Categories (paying customers):\")\n\n# spending ranges\nsmall_spenders = non_zero[non_zero <= q1_buyers]\nmedium_spenders = non_zero[(non_zero > q1_buyers) & \n                                    (non_zero <= q3_buyers)]\nlarge_spenders = non_zero[(non_zero > q3_buyers) & \n                                   (non_zero <= upper_bound_buyers)]\noutlier_spenders = non_zero[non_zero > upper_bound_buyers]\n\nprint(f\"  - Small spenders (<= ${q1_buyers:,.0f}): {len(small_spenders)} customers\")\nprint(f\"  - Medium spenders (${q1_buyers:,.0f} - ${q3_buyers:,.0f}): {len(medium_spenders)} customers\")\nprint(f\"  - Large spenders (${q3_buyers:,.0f} - ${upper_bound_buyers:,.0f}): {len(large_spenders)} customers\")\nprint(f\"  - VIP spenders (> ${upper_bound_buyers:,.0f}): {len(outlier_spenders)} customers\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Bar graph for spending categories\ncategories = ['Small', 'Medium', 'Large', 'VIP']\ncustomer_counts = [6066, 11923, 3151, 2845]\n\n# Create bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(categories, customer_counts, \n               color=['blue', 'green', 'red', 'yellow'])\n\nplt.title('Customer Spending Categories')\nplt.xlabel('Spending Category')\nplt.ylabel('Number of Customers')\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Missing Data Analysis","metadata":{}},{"cell_type":"code","source":"# Missing data analysis\nprint(\"=\" * 60)\nprint(\"-\"*12,\"MISSING DATA ANALYSIS\",\"-\"*12)\nprint(\"=\" * 60)\n\n# Check for 'not available in demo dataset' values\nzero_value_cols = []\npartial_missing_cols = []\n\nprint(\"-\"*12,\"Partially Filled:\",\"-\"*12)\nfor col in train.columns:\n    if train[col].dtype == 'object':  # Only check text columns\n        not_available = (train[col] == 'not available in demo dataset').sum()\n        if not_available == train.shape[0]:  # All values are 'not available'\n            zero_value_cols.append(col)\n        elif not_available > 0:  # Some values are 'not available'\n            partial_missing_cols.append((col, not_available))\n            print(f\"{col}: {not_available} 'not available in dataset' values ({not_available/train.shape[0]*100:.1f}%)\")\n\nprint(f\"\\n Completely empty columns ({len(zero_value_cols)}):\\n {zero_value_cols}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null/missing values\nmissing_percent = (train.isnull().sum() / len(train) * 100).sort_values(ascending=False)\nmissing_columns = missing_percent[missing_percent > 0]\n\nprint(\"\\n Missing Values % Column-wise:\")\nprint(\"=\" * 40)\n\nif len(missing_columns) > 0:\n    for column, percentage in missing_columns.items():\n        print(f\" {column}: {percentage:.1f}%\")\n    \n    # Visualize missing data\n    plt.figure(figsize=(12, 6))\n    top_missing = missing_columns.head(10)\n    plt.bar(range(len(top_missing)), top_missing.values)\n    plt.title('Missing Data % by Column')\n    plt.xlabel('Columns')\n    plt.ylabel('Missing Percentage (%)')\n    plt.xticks(range(len(top_missing)), top_missing.index, rotation=45)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Visualizing Features","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ngroup = train.groupby('deviceType').size().rename('Count').reset_index()\n\n# pie chart Device category\nplt.figure(figsize=(6, 4))\nplt.pie(group['Count'], \n         labels=group['deviceType'], \n         autopct='%1.2f%%')\n\nplt.title('Device Category')\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"continents = train['geoNetwork.continent'].value_counts()\nnum_bars = len(continents)\nrandom_colors = np.random.rand(num_bars, 3) \n\n# bar chart for Continent Distribution \nplt.figure(figsize=(8, 6))\nplt.bar(continents.index, continents.values,color=random_colors)\n\nplt.title('Continent Distribution')\nplt.xlabel('Continent')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Filling the missing values and Also Changing \"(not set)\" and \"not available in dataset\" columns to Nan.\n\n#Defined Function\ndef fill_na(df):   \n    df['pageViews'] = df['pageViews'].fillna(1)\n    df['new_visits'] = df['new_visits'].fillna(0)\n    df['totals.bounces'] = df['totals.bounces'].fillna(0)\n    \n    df['trafficSource.isTrueDirect'] = df['trafficSource.isTrueDirect'].fillna(False)\n    \n    # filling all 'not set' and 'not available in demo dataset' \n    df.loc[(df['geoNetwork.city'] == \"(not set)\") | \n    (df['geoNetwork.city'] == \"not available in demo dataset\"),'geoNetwork.city'] = np.nan\n    \n    df['geoNetwork.city'] = df['geoNetwork.city'].fillna(\"NaN\")\n\n    df.loc[(df['geoNetwork.metro'] == \"(not set)\") | \n    (df['geoNetwork.metro'] == \"not available in demo dataset\"),'geoNetwork.metro'] = np.nan\n    \n    df['geoNetwork.metro'] = df['geoNetwork.metro'].fillna(\"NaN\")\n\n    df.loc[(df['geoNetwork.region'] == \"(not set)\") | \n    (df['geoNetwork.region'] == \"not available in demo dataset\"),'geoNetwork.region'] = np.nan\n    \n    df['geoNetwork.region'] = df['geoNetwork.region'].fillna(\"NaN\")\n\n    df.loc[df['geoNetwork.continent'] == \"(not set)\", 'geoNetwork.continent'] = np.nan\n    \n    df['geoNetwork.continent'] = df['geoNetwork.continent'].fillna(\"NaN\")\n\n    return df\n\ntrain = fill_na(train)\ntest = fill_na(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.1 Dropping all the defined empty columns ","metadata":{}},{"cell_type":"code","source":"#zero_value_cols defined earlier\nprint(zero_value_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zero values are 100% empty and No value are more than 60% \nNo_value = ['trafficSource.adContent',\n         'trafficSource.keyword',\n         'trafficSource.referralPath',\n         'trafficSource.adwordsClickInfo.slot',\n         'trafficSource.adwordsClickInfo.isVideoAd',\n         'trafficSource.adwordsClickInfo.adNetworkType',\n         'trafficSource.adwordsClickInfo.page'\n        ]\ntrain.drop(columns=(No_value + zero_value_cols),inplace =True)\ntest.drop(columns=(No_value + zero_value_cols),inplace =True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 Dropping constanst columns\n","metadata":{}},{"cell_type":"code","source":"constant_cols = [col for col in train.columns if train[col].nunique() == 1]\nprint(f'Columns : {constant_cols}')\n\n\ntrain.drop(constant_cols, axis=1, inplace=True)\ntest.drop(constant_cols, axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 Dropping High cardinality Columns","metadata":{}},{"cell_type":"code","source":"# Finding High cardinality features\n\n# Calculate the number of unique values for each categorical column\ncardinality = train.nunique()\n\nprint(\"Cardinality of each column:\")\nprint(cardinality.sort_values(ascending=False))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#finding correlation between sesionStart and sessionId before dropping:\n\ncorrelation = train['sessionStart'].corr(train['sessionId'])\nprint(f\"Correlation: {correlation:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#finding correlation between sesionStart and date before dropping one of them:\n\ncorrelation = train['sessionStart'].corr(train['date'])\nprint(f\"sessionStart vs date correlation: {correlation:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dropping sessionId and userId (identifier columns):\n\ntrain.drop(columns=['sessionId', 'userId'], inplace=True)\ntest.drop(columns=['sessionId', 'userId'], inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Heatmap\n\n- Shows how Numerical features are correalted with PurchaseValue and with each other ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# selecting numerical columns\nnumeric_cols = train.select_dtypes(include=[np.number]).columns\ncorr_matrix = train[numeric_cols].corr()\n\n# correlation heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, \n           annot=True, \n           cmap='coolwarm', \n           center=0, \n           square=True, \n           fmt='.2f')\n\nplt.title('Correlation Matrix: Numerical Features')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5.  Data Preprocessing & Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Extracting Date & time features","metadata":{}},{"cell_type":"code","source":"# Feature Engineering sessionStart column\ndef date_time_features(df):\n    # Converting to datetime\n    df['sessionStart_dt'] = pd.to_datetime(df['sessionStart'], unit='s')  #sessionStart was represented in Unix timestamp\n    \n    df['week_day'] = df['sessionStart_dt'].dt.dayofweek\n    df['month'] = df['sessionStart_dt'].dt.month\n    df['hour'] = df['sessionStart_dt'].dt.hour\n    df['is_weekend'] = (df['week_day'] >= 5).astype(int)\n    \n    # Time periods\n    df['period'] = pd.cut(df['hour'],bins=[0, 6, 12, 18, 24], \n                              labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n    \n    # Converting categorical to numerical \n    df['day_period'] = df['period'].cat.codes\n    \n    # Droping intermediate columns and Date\n    df.drop(columns=['sessionStart_dt', 'period','date'], inplace=True)\n    \n    return df\n\ntrain = date_time_features(train)\ntest = date_time_features(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2 Splitting the Dataset","metadata":{}},{"cell_type":"code","source":"X = train.drop('purchaseValue', axis=1)\ny = train['purchaseValue']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Validation set shape: {X_val.shape}\")\nprint(f\"Target training shape: {y_train.shape}\")\nprint(f\"Target validation shape: {y_val.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# numerical and categorical columns\nnum_cols = X_train.select_dtypes(include=np.number).columns.tolist()\ncat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"\\nCategorical columns:\", cat_cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Model Training","metadata":{}},{"cell_type":"markdown","source":"## 6.1 XGBoost","metadata":{}},{"cell_type":"code","source":"# def Xgbmodel():\n#     from sklearn.pipeline import Pipeline\n#     from sklearn.impute import SimpleImputer\n#     from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n#     from sklearn.compose import ColumnTransformer\n#     import xgboost as xgb\n    \n#     #num_features = ['sessionNumber','pageViews', 'totalHits', 'sessionStart','day_period', 'gclIdPresent']\n    \n#     sel_num = ['sessionNumber', 'pageViews', 'totalHits' ,'sessionStart','new_visits','month','hour'] \n    \n#     cat_features = ['browser','userChannel','geoNetwork.continent','geoNetwork.metro',\n#                    'geoNetwork.city','locationCountry', 'geoNetwork.region','geoNetwork.subContinent']\n\n#     #sel_cat =  ['browser','geoCluster','trafficSource','os','deviceType','userChannel',]\n\n\n#     # Creating transformers \n#     num_pipeline = Pipeline(steps=[\n#         ('imputer', SimpleImputer(strategy='median')),\n#         ('scaler', StandardScaler())\n#     ])\n    \n#     cat_pipeline = Pipeline(steps=[\n#         ('imputer', SimpleImputer(strategy='most_frequent')),\n#         ('encoder', OrdinalEncoder(\n#             handle_unknown='use_encoded_value', \n#             unknown_value=-1\n#         ))\n#     ])\n    \n#     # Preprocessor\n#     preprocessor = ColumnTransformer(\n#         transformers=[\n#             ('num', num_pipeline, sel_num),\n#             ('cat', cat_pipeline, cat_features)\n#         ],\n#         remainder='drop'\n#     )\n    \n#     # Complete pipeline\n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('regressor', xgb.XGBRegressor(\n#             learning_rate=0.15,  # Using eta value\n#             max_depth=8,\n#             min_child_weight=100,\n#             gamma=5,\n#             subsample=1,\n#             colsample_bytree=0.95,\n#             colsample_bylevel=0.35,\n#             reg_alpha=25,\n#             reg_lambda=25,\n#             n_jobs=-1,\n#             verbosity=0,\n#             random_state=42,\n#             n_estimators=100\n#         ))\n#     ])\n    \n#     return pipeline\n\n\n# XGB = Xgbmodel()\n# XGB.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Checking XGboost Model performance\n\n# # Predict on training set\n# y_train_pred = XGB.predict(X_train)\n# r2_train = r2_score(y_train, y_train_pred)\n\n# # Predict on validation set\n# y_val_pred = XGB.predict(X_val)\n# r2_val = r2_score(y_val, y_val_pred)\n\n# # Print both\n# print(f\"Training R² score: {r2_train:.4f}\")\n# print(f\"Validation R² score: {r2_val:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n\n# regressor = XGB.named_steps['regressor']\n# preprocessor = XGB.named_steps['preprocessor']\n\n# importances = regressor.feature_importances_\n\n# feature_names = preprocessor.get_feature_names_out()\n\n# feature_importance_series = pd.Series(importances, index=feature_names)\n\n# sorted_importances = feature_importance_series.sort_values(ascending=True)\n\n# plt.figure(figsize=(12, 10))\n# sorted_importances.plot(kind='barh')\n# plt.title('XGBoost Feature Importance from Pipeline')\n# plt.xlabel('Importance Score')\n# plt.ylabel('Features')\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"# def RFmodel():\n#     from sklearn.pipeline import Pipeline\n#     from sklearn.impute import SimpleImputer\n#     from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n#     from sklearn.compose import ColumnTransformer\n#     from sklearn.ensemble import RandomForestRegressor as RF\n    \n#     num_features = ['sessionNumber','pageViews', 'totalHits', 'sessionStart',\n#                    'day_period','hour', 'week_day']\n    \n#     cat_features = ['userChannel', 'geoNetwork.continent', 'geoNetwork.metro',\n#                    'geoNetwork.city', 'locationCountry', 'geoNetwork.region',\n#                    'geoNetwork.subContinent']\n\n#     # Numerical preprocessing pipeline\n#     num_transformer = Pipeline(steps=[\n#         ('imputer', SimpleImputer(strategy='median')),\n#         ('scaler', StandardScaler())\n#     ])\n\n#     # Categorical Preprocessing pipeline\n#     cat_transformer = Pipeline(steps=[\n#         ('imputer', SimpleImputer(strategy='most_frequent')),\n#         ('encoder', OrdinalEncoder(\n#             handle_unknown='use_encoded_value', \n#             unknown_value=-1\n#         ))\n#     ])\n    \n#     # Column Transformer \n#     preprocessor = ColumnTransformer(\n#         transformers=[\n#             ('num', num_transformer, num_features),\n#             ('cat',cat_transformer, cat_features)\n#         ],\n#         remainder='drop'\n#     )\n    \n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('regressor', RF(\n#             n_estimators=250,\n#             max_depth=25,\n#             min_samples_split=8,\n#             min_samples_leaf=4,\n#             max_features=0.2,\n#             random_state=60,\n#             n_jobs=-1  # Use all available cores\n#         ))\n#     ])\n    \n#     return pipeline\n\n\n# RF_model = RFmodel()\n# RF_model.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Checking ExtraTreesRegressor Model performance\n\n# # Predict on training set\n# y_train_pred = RF_model.predict(X_train)\n# r2_train = r2_score(y_train, y_train_pred)\n\n# # Predict on validation set\n# y_val_pred = RF_model.predict(X_val)\n# r2_val = r2_score(y_val, y_val_pred)\n\n# # Print both\n# print(f\"Training R² score: {r2_train:.4f}\")\n# print(f\"Validation R² score: {r2_val:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.3 ExtraTreesRegressor","metadata":{}},{"cell_type":"code","source":"def ETRmodel():\n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.impute import KNNImputer\n    from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.ensemble import ExtraTreesRegressor as ETR\n    \n    # Feature definitions\n    num_features = ['sessionNumber', 'pageViews', 'totalHits', 'sessionStart',\n                    'week_day', 'month', 'hour']\n\n    # sel_num = ['sessionNumber', 'pageViews', 'totalHits' ,'sessionStart','new_visits','month','hour'] \n    # new_num = ['sessionNumber','pageViews','date', 'totalHits', 'sessionStart',\n    #        'gclIdPresent','totals.visits']\n\n    \n#     cat_features = ['browser','userChannel','geoNetwork.continent','geoNetwork.metro',\n#                    'geoNetwork.city','locationCountry', 'geoNetwork.region','geoNetwork.subContinent']\n\n#     #sel_cat =  ['browser','geoCluster','trafficSource','os','deviceType','userChannel',]\n    \n    # Numerical preprocessing pipeline\n    num_transformer = Pipeline(steps=[\n        ('imputer', KNNImputer(n_neighbors=10)),\n        ('scaler', StandardScaler())\n    ])\n    \n    # Column Transformer \n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_transformer, num_features),\n        ],\n        remainder='drop'\n    )\n    \n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('regressor', ETR(\n            n_estimators=260,\n            max_depth=30,\n            max_features=0.2,\n            random_state=60,\n            min_samples_split=3,\n            n_jobs=-1\n        ))\n    ])\n    \n    return pipeline\n\n\nETR_model = ETRmodel()\nETR_model.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Checking ExtraTreesRegressor Model performance\n\n# Predict on training set\ny_train_pred = ETR_model.predict(X_train)\nr2_train = r2_score(y_train, y_train_pred)\n\n# Predict on validation set\ny_val_pred = ETR_model.predict(X_val)\nr2_val = r2_score(y_val, y_val_pred)\n\n# Print both\nprint(f\"Training R² score: {r2_train:.4f}\")\nprint(f\"Validation R² score: {r2_val:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Feature Importance for ExtraTreesRegressor\n\nregressor = ETR_model.named_steps['regressor']\npreprocessor = ETR_model.named_steps['preprocessor']\n\nimportances = regressor.feature_importances_\n\nfeature_names = preprocessor.get_feature_names_out()\n\nfeature_importance_series = pd.Series(importances, index=feature_names)\n\nsorted_importances = feature_importance_series.sort_values(ascending=True)\n\nplt.figure(figsize=(10, 6))\nsorted_importances.plot(kind='barh')\nplt.title('Extra Trees Regressor Feature Importance')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.4 Model Comparison and Analysis \n\nBased on the results, **ExtraTreesRegressor (ETR)** performed best among the three models:\n\n| Model | Training R2 | Validation R2 | Generalization Gap |\n|-------|-------------|---------------|---------------------|\n| **ETR** | 0.9838 | **0.4135** | 0.5703 |\n| RF    | 0.6139 | 0.2043 | 0.4096 |\n| XGB   | 0.8859 | 0.1067 | 0.7792 |\n\n---\n\n### ExtraTreesRegressor got:\n\n- **Best Validation Performance**: Achieved the highest R2 (0.4135) on unseen data.\n- **Better Generalization**: While it still overfits, the generalization gap is more manageable than XGBoost.\n- **Randomization Benefits**: The extreme randomization in ExtraTrees helps capture complex patterns without memorizing noise.\n\n---\n\n### XGBoost Performed Poorly:\n\n- **Severe Overfitting**: Massive gap between training (0.8859) and validation (0.1067) R2 score can be seen.\n\n","metadata":{}},{"cell_type":"markdown","source":"# 7. Final Submission","metadata":{}},{"cell_type":"code","source":"# # Xgboost\n# # fitting on the whole dataset now\n# XGB.fit(X, y)\n# y_pred = XGB.predict(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # RandomForestRegressor\n\n# RF_model.fit(X, y)\n# y_pred = RF_model.predict(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ExtraTreesRegressor\nETR_model.fit(X, y)\ny_pred = ETR_model.predict(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#creating final submission fie:\nsubmission = pd.DataFrame({\"id\": np.arange(len(y_pred)),\"purchaseValue\": y_pred})\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}